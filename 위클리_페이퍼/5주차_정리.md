# **9.1~9.5(5주차) 위클리 페이퍼**

## 질문
- 딥러닝과 머신러닝 간의 포함관계에 대해 설명해주세요.
- 딥러닝의 성능향상을 위해 고려하는 하이퍼파라미터의 종류에는 어떤 것들이 있는지 설명해주세요.

---
## 1. 딥러닝과 머신러닝 간의 포함관계에 대해 설명해주세요.

### 딥러닝이란?
- AI의 한 분야로, 인공신경망(Artificial Neural Network, ANN)을 기반으로 한 기계 학습(Machine Learning, ML)기법
- 다층 신경망(Deep Neural Network, DNN)을 활용하여 데이터를 학습하고, 복잡한 패턴을 인식하거나 예측하는 데 사용
### 머신러닝이란?
- 컴퓨터와 기계가 인간의 학습 방식을 모방하여 작업을 효율적으로 수행하고, 경험과 더 많은 데이터에 대한 노출을 통해 성능과 정확도를 향상시킬 수 있도록 하는 데 중점을 둔 AI의 한 분야
- 다양한 알고리즘을 포함하며, 지도학습, 비지도학습, 강화학습 등 접근법으로 데이터를 학습함
- 다양한 알고리즘에는 선형 회귀, 결정트리, SVM 등이 있음
### 어떤 관계가 있을까?

<img width="772" height="417" alt="image" src="https://github.com/user-attachments/assets/03625be1-f501-4368-89f6-ac144055856f" />

딥러닝은 머신러닝 방법 중 하나이다. </br>
**모든 딥러닝은 머신러닝이지만, 모든 머신러닝이 딥러닝은 아님**

---

## 2. 딥러닝의 성능향상을 위해 고려하는 하이퍼파라미터의 종류에는 어떤 것들이 있는지 설명해주세요.

  
### 하이퍼파라미터란?
- 모델이 직접 학습하지 않고 사람이 직접 설정해야 하는 값
- 성능을 크게 좌우하며, 잘못설정하면 학습이 불안정하거나 과적합될 수 있음

### 주요 하이퍼파라미터
- 크게 모델 구조, 학습과정, 정규화 관련 하이퍼파라미터가 있음 
- **모델 구조**
  - 은닉층 수 num_layers
  - 뉴런 수 hidden_units
  - 활성화 함수 activation (ReLu, Sigmoid, Tanh, LeakyReLU 등)
  - CNN계열에서 필터 연산 세부 설 (kernel_size, stride, padding)
- **학습과정**
  - 학습률 learning_rate (lr)
  - 배치 batch_size
- **정규화 관련**
  - 드롭아웃 비율 dropout
  - 가중치 감소 weight_decay
 
### 튜닝
 문제 상황 | 튜닝 우선순위 |
| :--- | :--- |
| **학습이 느릴 때** | 1. **Learning rate 증가**: 학습 속도를 직접적으로 높임<br>2. **Optimizer 변경**: `SGD` → `Adam` 등으로 변경하여 수렴 속도를 개선<br>3. **Batch size 증가**: 하드웨어가 허용하는 선에서 크기를 키워 연산 효율을 높임 |
| **정확도가 낮을 때 (과소적합)** | 1. **Hidden units 증가 / Layer 추가**: 모델의 복잡도를 높여 표현력을 키움<br>2. **Epochs 증가**: 더 오래 학습시켜 모델이 충분히 데이터 패턴을 익히게 함<br>3. **Dropout 감소 / 정규화 계수(λ) 감소**: 모델에 가해진 제약을 완화하여 더 자유롭게 학습하게 함 |
| **과적합일 때 (훈련 정확도만 높음)** | 1. **Dropout 증가 / 정규화 계수(λ) 조정**: 모델의 복잡도를 낮추는 가장 효과적인 방법 <br>2. **데이터 증강 (Data Augmentation)**: 더 많은 학습 데이터를 만들어 일반화 성능 높임<br>3. **Hidden units 감소 / Layer 감소**: 모델의 크기를 줄여 과적합 가능성을 낮춤 |
| **최적화가 안 될 때 (손실이 줄지 않거나 발산)** | 1. **Learning rate 감소**: 학습률을 낮춰 안정적으로 최적점을 찾게함<br>2. **Batch size 증가**: 불안정한 그래디언트를 완화시켜 학습을 안정<br>3. **그래디언트 클리핑 (Gradient Clipping)**: 그래디언트 폭발 방지 |
